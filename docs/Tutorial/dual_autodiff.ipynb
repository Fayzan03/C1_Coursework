{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Notebook\n",
    "\n",
    "This is a basic tutorial showcasing the various features of the `dual_autodiff` Python package. \n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "Once the package has been installed, import the dual_autodiff package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dual_autodiff in c:\\users\\fayza\\mphil_dis\\c1\\fm565\\venv\\c1_cwk\\lib\\site-packages (0.1.dev15+g2c0505d.d20241209)\n",
      "Requirement already satisfied: numpy in c:\\users\\fayza\\mphil_dis\\c1\\fm565\\venv\\c1_cwk\\lib\\site-packages (from dual_autodiff) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dual_autodiff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install the dual_autodiff package\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall dual_autodiff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdual_autodiff\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dual_autodiff'"
     ]
    }
   ],
   "source": [
    "# Install the dual_autodiff package\n",
    "%pip install dual_autodiff\n",
    "\n",
    "import dual_autodiff as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dual number by calling the `Dual` class and passing a real and dual part as arguments. We can then return the real and dual parts in two different ways, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dual number\n",
    "x=df.Dual(2,1)\n",
    "\n",
    "# Print the real and dual parts, either using the class attributes or designated member functions\n",
    "print(x.real)\n",
    "print(x.du())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic arithmetical operations to dual numbers as we would with real numbers. This includes: addition, subtraction, multiplication, and division on either the LHS or RHS of the dual number; operate-and-assign operators; raising to a (real) exponent; comparison operators; and unary operators (i.e. inverting, or putting a '-' sign in front of a dual number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.Dual(3,4)\n",
    "# addition\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division\n",
    "print(y/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply such operations to real numbers on both the LHS and RHS of a dual number.\n",
    "print(y-5, 5-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The operate-and-assign operators are also defined similarly.\n",
    "\n",
    "x*=10\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponentiation of dual numbers is defined for purely real exponents (i.e. the dual part must be 0).\n",
    "\n",
    "z=df.Dual(4,9)\n",
    "\n",
    "print(z**0.5)\n",
    "print(z**df.Dual(0.5,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check for (in)equivalence of dual numbers. \n",
    "\n",
    "x==z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: '~' is the inversion operator (equivalent to raising an dual number to an exponent of (-1)).\n",
    "\n",
    "print(~z)\n",
    "print(z**(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dual` class also defines several common mathematical functions (see documentation for details) that can be extended to dual numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=df.Dual(np.pi/2, 1)\n",
    "\n",
    "print(a.sin())\n",
    "\n",
    "print(df.Dual(1,5).exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - using the `Dual` class to compute derivatives\n",
    "\n",
    "Dual numbers allow us to compute gradients to high levels of accuracy via the technique of Automatic Differentiation. \n",
    "\n",
    "This follows from Taylor's Theorem,\n",
    "$ f(a + b \\epsilon) = f(a) + f^\\prime(a) (b \\epsilon)$\n",
    "where higher-order terms vanish since $\\epsilon^2=0$. Thus the gradient at $x=a$ can be found by taking the dual part of $ f(a + \\epsilon) $.\n",
    "\n",
    "Below, we compute the gradient of $ f(x) = log(sin(x)) + x^2 cos(x) $ at $x=1.5$ using \n",
    " - dual numbers\n",
    " - the analytical derivative\n",
    " - numerically, with decreasing step size\n",
    "\n",
    "and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I compute the derivative of the function using dual numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that we would like to differentiate at x==1.5. \n",
    "x=df.Dual(1.5,1)\n",
    "output = (x.sin()).log() + (x**2) * x.cos()\n",
    "\n",
    "# Compute the gradient using Taylors' Theorem\n",
    "grad_dual=output.dual\n",
    "print(grad_dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, I compute the analytical derivative of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to the analytical result\n",
    "def f_prime(x):\n",
    "    return 1/np.tan(x) + 2*x*np.cos(x) - x**2 * np.sin(x)\n",
    "\n",
    "grad_anal=f_prime(1.5)\n",
    "print(grad_anal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This result agrees with the gradient obtained using dual numbers at the machine precision level (up to 1e-16)! This shows that the dual number method is very accurate, which is highly useful for performing automatic differentiation.\n",
    "\n",
    "Thirdly, for comparison, I compute the numerical derivative of the function using a decreasing step size. I have also plotted the results and included the dual/analytical derivative for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function f to compute the numerical gradient\n",
    "def f(x):\n",
    "    y = np.log(np.sin(x)) + x**2 *np.cos(x)\n",
    "    return y\n",
    "\n",
    "# Compute the numerical gradient at x==1.5    \n",
    "x=1.5  \n",
    "\n",
    "grad_num=[]\n",
    "for i in range(15):\n",
    "    eps=10**(-i)\n",
    "    y = (f(x+eps) - f(x))/eps\n",
    "    grad_num.append(y)\n",
    "\n",
    "# Plot the gradient at each step size\n",
    "step_size=[10**(-i) for i in range(15)]\n",
    "plt.plot(step_size, grad_num, marker='x', label=\"Numerical\")\n",
    "plt.xlabel(\"Step size\")\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"Gradient at $x=1.5$\")\n",
    "plt.title(\"Derivative of $ f(x) = log(sin(x)) + x^2 cos(x) $ at $x=1.5$\")\n",
    "plt.hlines(grad_dual, xmin=0, xmax=1, color='r', ls=\"-.\", label= \"Dual\")\n",
    "plt.hlines(grad_anal, xmin=0, xmax=1, color='g', ls=\"--\", label= \"Analytical\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a good approximation to the derivative, but if we examine the errors, we find that it is not as accurate as the dual number method. I have plotted the error below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error at each step size\n",
    "step_size=[10**(-i) for i in range(15)]\n",
    "plt.plot(step_size, np.abs(grad_num-grad_anal), marker='x', label=\"Numerical\")\n",
    "plt.xlabel(\"Step size\")\n",
    "plt.ylabel(\"Absolute value of Error\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#plt.ylim(10**(-9), 10**(0))\n",
    "plt.title(\"|Error| in derivative of $ f(x) = log(sin(x)) + x^2 cos(x) $ at $x=1.5$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the numerical method is limited by the step size, which can introduce errors if it becomes too large (because the higher order terms in the Taylor expansion are not yet small enough) or too small (due to rounding error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cythonized Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dual_autodiff_x as dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dfx.Dual(1.5,1)\n",
    "x**50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Define a function that we would like to differentiate. \n",
    "output = (x.sin()).log() + (x**2) * x.cos()\n",
    "\n",
    "# Compute the gradient at x==1.5 using Taylors' Theorem\n",
    "x=dfx.Dual(1.5,1)\n",
    "grad_dual=output.dual\n",
    "print(grad_dual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c1_cwk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
